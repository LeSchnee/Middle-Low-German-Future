{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50815251",
   "metadata": {},
   "source": [
    "This script reads a txt file created with the *Tagger* function of *Collatinus 11* and creates a horizontal dataframe with one token per line, including the following informations and saves it as a csv file.\n",
    "- sentence_nr (Satznummer; 1:1, 1:2, ...n:n)\n",
    "- Sentence (Kontext),\n",
    "- morphological Annoations, \n",
    "- Lemma,\n",
    "- Translation (german or English) and\n",
    "- Future (yes|no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfe899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads annotated file\n",
    "file1 = open('Collatinus_annotated_files/luk_2_annotated.txt', 'r', encoding = \"UTF-8\")\n",
    "Lines = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters out non-text lines\n",
    "UsefulLines = [x for x in Lines if not x.startswith((\",\", \"\\n\", \" \\n\", \"snt\", \"avec la proba\", \"Deuxième choix avec la proba\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterates through lines and collects relevant information into lists to create dataframe\n",
    "tokList = []\n",
    "tokCleanList = []\n",
    "posList = []\n",
    "postagList=[]\n",
    "\n",
    "ZeilenCount = 0\n",
    "for line in UsefulLines:\n",
    "    if line.endswith(\"non trouvé\\n\"):\n",
    "            #print(\"NOT TAGGED\", line)\n",
    "            tokList.append(line[:-11])\n",
    "            tokCleanList.append(line[:-11])\n",
    "            posList.append(\"NON TROUVÉ\")\n",
    "\n",
    "    elif line.startswith(\"—>\"):\n",
    "            #print(\"posTag:\", line)\n",
    "            posList.append(line)\n",
    "\n",
    "    else:\n",
    "            #print(\"tok:\", line)\n",
    "            tokList.append(line)\n",
    "            tokCleanList.append(str.split(line)[0])\n",
    "            postagList.append(' '.join(str.split(line)[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e14af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the available information into morph. annotation, Lemma and translation\n",
    "morphList = []\n",
    "lemmaList = []\n",
    "transList = []\n",
    "\n",
    "for line in posList:\n",
    "    a = line[3:]\n",
    "    a = re.split(\":|—\", a)\n",
    "    #print(len(a))\n",
    "    if len(a)==3:\n",
    "        b = a[0].split(\",\")\n",
    "        lemmaList.append(b[0])\n",
    "        transList.append(a[1])\n",
    "        morphList.append(a[2][:-2])\n",
    "    else: \n",
    "        morphList.append(\"-\")\n",
    "        lemmaList.append(\"-\")\n",
    "        transList.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates column: Futur (FUTUR|---)\n",
    "\n",
    "futList = []\n",
    "for element in posList:\n",
    "    if \"futur\" in element:\n",
    "        futList.append(\"FUTUR\")\n",
    "    else:\n",
    "        futList.append(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33451962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erstellt datenset aus den Listen\n",
    "#data_tuples = list(zip(sentList, sentsentList,tokList, tokCleanList,posList, morphList, lemmaList, lemmaCleanList, transList, futList))\n",
    "\n",
    "# df = pd.DataFrame(data_tuples, columns=['SatzNr','Satz', 'Token', 'Token_clean','Annotation', 'MorphAnno', 'Lemma', 'Lemma_clean', 'Translation', 'Futur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataframe from lists - without line and line nr, will be added afterwards\n",
    "data_tuples = list(zip(tokList, tokCleanList, postagList, posList, morphList, lemmaList,  transList, futList))\n",
    "\n",
    "df = pd.DataFrame(data_tuples, columns=['Token', 'Token_clean', 'pos_Tag','Annotation', 'MorphAnno', 'Lemma', 'Translation', 'Futur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547faee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in non-annotated file (for line nrs)\n",
    "file1 = open('txt_files_vulgata/Luk_2.txt', 'r', encoding = \"UTF-8\")\n",
    "Lines = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f950f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "sentList = []\n",
    "sentNrList = []\n",
    "verseNrList = []\n",
    "for element in Lines:\n",
    "    if not element == ' \\n':\n",
    "        if not element == '\\n':\n",
    "            counter +=1\n",
    "            splitel = element.split()\n",
    "            sentence = ' '.join(splitel[1:])\n",
    "            versenr = splitel[0]\n",
    "            \n",
    "            #print(len(splitel[1:]))\n",
    "            for i in range(len(splitel[1:])):\n",
    "                sentList.append(sentence)\n",
    "                verseNrList.append(versenr)\n",
    "                sentNrList.append(counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723dbb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataframe with ID, line-nr and line\n",
    "data_tuples = list(zip(sentNrList, verseNrList, sentList))\n",
    "df1 = pd.DataFrame(data_tuples, columns=['ID', 'Satznummer', 'Satz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges the two dataframes\n",
    "df2 = pd.concat([df1, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32bf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speichert als csv-Datei\n",
    "df2.to_csv('Luk_2_pos.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
